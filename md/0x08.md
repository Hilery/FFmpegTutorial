
## 0x08 使用 Core Animation 渲染视频帧

本篇教程仅仅将  [0x07](./0x07.md) 教程的渲染 CGImageRef 改为，先将 CGImageRef 转成 UIImage，然后使用 UIImageView 渲染而已。

```objc
- (void)reveiveFrameToRenderer:(CGImageRef)cgImage
{
    UIImage *image = [UIImage imageWithCGImage:cgImage];

    dispatch_async(dispatch_get_main_queue(), ^{
        self.imgView.image = image;
    });
}
```

虽然看起来很简单，但实际上渲染的方式变了，CGImage 实际上是已经解过码的数据，然后构造成 UIImage，其实开源的图片缓存框架提升图片渲染性能的核心逻辑就是异步线程解码图片，然后然后创建 CGImage，再转成 UIImage，避免了在渲染前在主线程使用 CPU 解码，导致的卡顿！

## CGImage vs UIImage

WWDC 2012 Said:

- UIImage is a lightweight wrapper around CGImage
- CALayer also has CGImage as contents
- CGImage backed by file or data, eventually by bitmap
- Use UIImageView instead of drawing image directly (usually) 
  - CA can get the bitmap from the CGImage directly
  - Allow blending to happen on GPU
  - Built-in bitmap caching

CGImage 可以通过文件路径创建，可以通过原始数据（没有解码）创建 ，可以通过解码后的像素数据创建，可谓非常灵活。

UIImage 可以通过 CGImage 创建而来，当使用 UIImageView 渲染 UIImage 时，如果没解码会先在主线程解码，已经解过码，则会把 CGImage 作为 UIImageView 对应 layer 的 contents 。

通常情况下应当使用 UIImageView 替代其他的相关的 API 直接绘制，可以提高性能，的确如此，看看下面的测试数据就知道了！

## iPhone 7 plus 真机实测数据

以下数据是以 Xcode 提供的数据为标准，并且不开启 Performance 监控（MRAppDelegate 里注释掉 [PerformanceWrapper show]; ），使用 demo 里提供的带时间水印的视频，像素格式使用 MR_PIX_FMT_MASK_0RGB 测量而来。

停留在首页时记录数据为：CPU 占用 1%，内存 17.5M；

进入 0x08ViewController 之后，观察渲染情况；

 - 第 20s 视频： CPU 占用 37%，内存 30.7M；
 - 第 40s 视频： CPU 占用 37%，内存 31M；
 - 第 61s 视频： CPU 占用 3%，内存 27.1M；

从开始播放到结束：

CPU 最高 46%， 内存最高 34.2M;

从数据来看，使用 Core Animation 渲染视频在 CPU 和内存使用上均比使用 [Core Graphics](./0x07.md) 要少一些，因为是把通过解码后的像素数据创建的CGImage 转成了 UIImage，然后使用 UIImageView 渲染的，这一过程不会申请后备存储器，避免了内存的浪费，也避免了将  CGImage 像素数据拷贝到后备存储器的过程，所以也会减少对 CPU 的使用！尽管如此，当前 CPU 的消耗还是挺大的，需要继续寻找其他的渲染方式，下一篇我们将使用 [Core Image](./0x09.md) 渲染试试。

另外换成 MR_PIX_FMT_MASK_RGB555 格式后内存变化很小，但是 CPU 使用却明显上升了，这跟 Core Graphics  的表现不一样，我猜测可能是 UIImageView 内部使用 CPU 做了像素格式的转换，可自行测试。
