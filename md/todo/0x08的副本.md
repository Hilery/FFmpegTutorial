
## 0x08 使用 Core Animation 渲染视频帧

本篇教程在  [0x06](./0x06.md) 教程的基础之上增加了两个功能：

- 视频像素格式转换
- 使用 UIImageView 渲染视频帧

## 思路分析

若要使用 UIImageView 去渲染视频帧，那么必须要将 AVFrame 转成 UIImage 才行，了解到 RGB 像素格式的 Bitmap 可以创建 CGImage，而 CGImage 又很容易转成 UIImage，因此使用 UIImageView 渲染视频帧的前提成将视频像素格式转换成 RGB！好在 FFmpeg 提供了  **libswscale** 库，这个库就是用来做视频像素格式转换，图像缩放的。

- 需要提供一个类封装下 libswcale 库提供的函数，负责像素格式的转换，在解码后直接调用即可；

- 需要提供一个工具类将 RGB 像素格式的 AVFrame 转换成 UIImage，封装下转换过程；
- 需要将转换好的 UIImage 传给调用者，这里选择代理模式；

## 核心代码

### 像素格式转换类：FFVideoScale0x07 

结合 libswscale 库的使用方式和参数，封装后提供了一个初始化方法，传递必要的参数：

```objc
/// @param srcPixFmt 原帧像素格式
/// @param dstPixFmt 目标帧像素格式
/// @param picWidth 图像宽度
/// @param picHeight 图像高度
- (instancetype)initWithSrcPixFmt:(int)srcPixFmt
                        dstPixFmt:(int)dstPixFmt
                         picWidth:(int)picWidth
                        picHeight:(int)picHeight;
```

需要转换时调用：

```objc
/// @param inF 需要转换的帧
/// @param outP 转换的结果[不要free相关内存，通过ref/unref的方式使用]
- (BOOL) rescaleFrame:(AVFrame *)inF out:(AVFrame *_Nonnull*_Nonnull)outP;
```

libswcale 库使用步骤：

```objc
//初始化
self.sws_ctx = sws_getContext(picWidth, picHeight, srcPixFmt, picWidth, picHeight, dstPixFmt, SWS_POINT, NULL, NULL, NULL);
//分配内存空间
self.frame = av_frame_alloc();
av_image_fill_linesizes(out_frame->linesize, out_frame->format, out_frame->width);
av_image_alloc(out_frame->data, out_frame->linesize, self.picWidth, self.picHeight, self.dstPixFmt, 1);
//转换
int ret = sws_scale(self.sws_ctx, (const uint8_t* const*)inF->data, inF->linesize, 0, inF->height, out_frame->data, out_frame->linesize);
```



### AVFrame 转 UIImage 工具类：MRConvertUtil

AVFrame 的像素格式必须是 AV_PIX_FMT_RGB24 才能调用这个类进行转换！转成 CGImage 的代码如下：

```objc
const UInt8 *rgb = frame->data[0];
const size_t bytesPerRow = frame->linesize[0];
const int w = frame->width;
const int h = frame->height;
const CFIndex length = bytesPerRow * h;

CGBitmapInfo bitmapInfo = kCGBitmapByteOrderDefault;
///需要copy！因为frame是重复利用的；里面的数据会变化！
CFDataRef data = CFDataCreate(kCFAllocatorDefault, rgb, length);
CGDataProviderRef provider = CGDataProviderCreateWithCFData(data);
CFRelease(data);
///颜色空间与 AV_PIX_FMT_RGB24 对应
CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();

CGImageRef cgImage = CGImageCreate(w,
                                   h,
                                   8,
                                   24,
                                   bytesPerRow,
                                   colorSpace,
                                   bitmapInfo,
                                   provider,
                                   NULL,
                                   NO,
                                   kCGRenderingIntentDefault);
CGColorSpaceRelease(colorSpace);
CGDataProviderRelease(provider);
```

### 改造播放器

1、在创建视频解码对象之后，创建像素格式转换对象

```objc
...
self.videoDecoder.name = @"videoDecoder";
self.videoScale = [self createVideoScaleIfNeed];
...

- (FFVideoScale0x07 *)createVideoScaleIfNeed {
    //未指定期望像素格式
    if (self.supportedPixelFormats == MR_PIX_FMT_MASK_NONE) {
        NSAssert(NO, @"supportedPixelFormats can't be none!");
        return nil;
    }
    
    //当前视频的像素格式
    const enum AVPixelFormat format = self.videoDecoder.pix_fmt;
    
    bool matched = false;
    MRPixelFormat firstSupportedFmt = MR_PIX_FMT_NONE;
    MRPixelFormat allFmts[] = {MR_PIX_FMT_YUV420P, MR_PIX_FMT_NV12, MR_PIX_FMT_NV21, MR_PIX_FMT_RGB24};
    for (int i = 0; i < sizeof(allFmts)/sizeof(MRPixelFormat); i ++) {
        const MRPixelFormat fmt = allFmts[i];
        const MRPixelFormatMask mask = 1 << fmt;
        if (self.supportedPixelFormats & mask) {
            if (firstSupportedFmt == MR_PIX_FMT_NONE) {
                firstSupportedFmt = fmt;
            }
            
            if (format == MRPixelFormat2AV(fmt)) {
                matched = true;
                break;
            }
        }
    }
    
    if (matched) {
        //期望像素格式包含了当前视频像素格式，则直接使用当前格式，不再转换。
        return nil;
    }
    
    if (firstSupportedFmt == MR_PIX_FMT_NONE) {
        NSAssert(NO, @"supportedPixelFormats is invalid!");
        return nil;
    }
    
    ///创建像素格式转换上下文
    FFVideoScale0x07 *scale = [[FFVideoScale0x07 alloc] initWithSrcPixFmt:format dstPixFmt:MRPixelFormat2AV(firstSupportedFmt) picWidth:self.videoDecoder.picWidth picHeight:self.videoDecoder.picHeight];
    return scale;
}

在接收到解码器解出的 frame 之后，进行转换：
...
else if (decoder == self.videoDecoder) {
  FrameQueue *fq = &pictq;

  AVFrame *outP = nil;
  if (self.videoScale) {
    if (![self.videoScale rescaleFrame:frame out:&outP]) {
      #warning TODO handle sacle error
    }
  } else {
    outP = frame;
  }

  Frame *af = NULL;
  if (NULL != (af = frame_queue_peek_writable(fq))) {
    av_frame_ref(af->frame, outP);
    frame_queue_push(fq);
  }
}
...
```

2、渲染线程转成UIImage，并代理出去

```objc
//定义代理协议
@protocol FFPlayer0x07Delegate <NSObject>

@optional
- (void)reveiveFrameToRenderer:(UIImage *)img;

@end
 
//通知委托者
if ([self.delegate respondsToSelector:@selector(reveiveFrameToRenderer:)]) {
  UIImage *img = [MRConvertUtil imageFromRGB24Frame:vp->frame];
  [self.delegate reveiveFrameToRenderer:img];
}
```

3、播放器的调用

```objc
//指定支持的像素格式，播放器内部就会自动转换了
player.supportedPixelFormats = MR_PIX_FMT_MASK_RGB24;
player.delegate = self;

//代理方法会在渲染线程通知有新的 UIImage 需要渲染
- (void)reveiveFrameToRenderer:(UIImage *)img
{
    dispatch_async(dispatch_get_main_queue(), ^{
        self.imgView.image = img;
    });
}
```



## 总结

值得一说的是，FFVideoScale0x07 的 **输入和输出类型一致**，不会提高对后续接口的对接难度，因为当原帧像素格式跟目标帧像素格式一样时，就不用转换了，这时后续接口是直接使用 AVFrame 的！其内部实现也同样精彩，为避免多次申请内存空间，转换过程仿造解码过程，复用了一个 AVFrame 对象，提高了内存使用效率！外部调用时完全遵照 AVFrame 引用计数的正常使用方式即可！

结合掩码设计，提供了几种常见的像素格式，在调用播放器的时候通过 supportedPixelFormats 指定即可，如果指定了多个，内部会优先匹配，如果不能匹配则自动选择其中一种进行格式转换！这一设计为后续篇章使用其他方法渲染做个铺垫，其他的渲染方式，需要的是别的像素格式，具体是哪些格式，请接着往后看吧！