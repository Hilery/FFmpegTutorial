
## 0x07 使用 Core Graphies 渲染视频帧

本篇教程在  [0x06](./0x06.md) 教程的基础之上增加了两个功能：

- 视频像素格式转换
- 使用 Core Graphies 渲染视频帧

## 思路分析

若要使用 Core Graphies API 去渲染视频帧，那么必须要将 AVFrame 转成 CGImage 才行，这里需要稍微深入的提一下，像素有很多种表示形式，你可以找到 <libavutil/pixfmt.h> 找到 FFmpeg 支持的所有格式，由于太多了，我把 RGB 相关的贴出来一起看下：

```
AV_PIX_FMT_RGB24,     ///< packed RGB 8:8:8, 24bpp, RGBRGB...
AV_PIX_FMT_BGR24,     ///< packed RGB 8:8:8, 24bpp, BGRBGR...
AV_PIX_FMT_BGR8,      ///< packed RGB 3:3:2,  8bpp, (msb)2B 3G 3R(lsb)
AV_PIX_FMT_BGR4,      ///< packed RGB 1:2:1 bitstream,  4bpp, (msb)1B 2G 1R(lsb), a byte contains two pixels, the first pixel in the byte is the one composed by the 4 msb bits
AV_PIX_FMT_BGR4_BYTE, ///< packed RGB 1:2:1,  8bpp, (msb)1B 2G 1R(lsb)
AV_PIX_FMT_RGB8,      ///< packed RGB 3:3:2,  8bpp, (msb)2R 3G 3B(lsb)
AV_PIX_FMT_RGB4,      ///< packed RGB 1:2:1 bitstream,  4bpp, (msb)1R 2G 1B(lsb), a byte contains two pixels, the first pixel in the byte is the one composed by the 4 msb bits
AV_PIX_FMT_RGB4_BYTE, ///< packed RGB 1:2:1,  8bpp, (msb)1R 2G 1B(lsb)
AV_PIX_FMT_ARGB,      ///< packed ARGB 8:8:8:8, 32bpp, ARGBARGB...
AV_PIX_FMT_RGBA,      ///< packed RGBA 8:8:8:8, 32bpp, RGBARGBA...
AV_PIX_FMT_ABGR,      ///< packed ABGR 8:8:8:8, 32bpp, ABGRABGR...
AV_PIX_FMT_BGRA,      ///< packed BGRA 8:8:8:8, 32bpp, BGRABGRA...
AV_PIX_FMT_RGB48BE,   ///< packed RGB 16:16:16, 48bpp, 16R, 16G, 16B, the 2-byte value for each R/G/B component is stored as big-endian
AV_PIX_FMT_RGB48LE,   ///< packed RGB 16:16:16, 48bpp, 16R, 16G, 16B, the 2-byte value for each R/G/B component is stored as little-endian
AV_PIX_FMT_RGB565BE,  ///< packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), big-endian
AV_PIX_FMT_RGB565LE,  ///< packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), little-endian
AV_PIX_FMT_RGB555BE,  ///< packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), big-endian   , X=unused/undefined
AV_PIX_FMT_RGB555LE,  ///< packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), little-endian, X=unused/undefined
AV_PIX_FMT_BGR565BE,  ///< packed BGR 5:6:5, 16bpp, (msb)   5B 6G 5R(lsb), big-endian
AV_PIX_FMT_BGR565LE,  ///< packed BGR 5:6:5, 16bpp, (msb)   5B 6G 5R(lsb), little-endian
AV_PIX_FMT_BGR555BE,  ///< packed BGR 5:5:5, 16bpp, (msb)1X 5B 5G 5R(lsb), big-endian   , X=unused/undefined
AV_PIX_FMT_BGR555LE,  ///< packed BGR 5:5:5, 16bpp, (msb)1X 5B 5G 5R(lsb), little-endian, X=unused/undefined
AV_PIX_FMT_RGB444LE,  ///< packed RGB 4:4:4, 16bpp, (msb)4X 4R 4G 4B(lsb), little-endian, X=unused/undefined
AV_PIX_FMT_RGB444BE,  ///< packed RGB 4:4:4, 16bpp, (msb)4X 4R 4G 4B(lsb), big-endian,    X=unused/undefined
AV_PIX_FMT_BGR444LE,  ///< packed BGR 4:4:4, 16bpp, (msb)4X 4B 4G 4R(lsb), little-endian, X=unused/undefined
AV_PIX_FMT_BGR444BE,  ///< packed BGR 4:4:4, 16bpp, (msb)4X 4B 4G 4R(lsb), big-endian,    X=unused/undefined
AV_PIX_FMT_BGR48BE,   ///< packed RGB 16:16:16, 48bpp, 16B, 16G, 16R, the 2-byte value for each R/G/B component is stored as big-endian
AV_PIX_FMT_BGR48LE,   ///< packed RGB 16:16:16, 48bpp, 16B, 16G, 16R, the 2-byte value for each R/G/B component is stored as little-endian
AV_PIX_FMT_GBRP,      ///< planar GBR 4:4:4 24bpp
AV_PIX_FMT_GBR24P = AV_PIX_FMT_GBRP, // alias for #AV_PIX_FMT_GBRP
AV_PIX_FMT_GBRP9BE,   ///< planar GBR 4:4:4 27bpp, big-endian
AV_PIX_FMT_GBRP9LE,   ///< planar GBR 4:4:4 27bpp, little-endian
AV_PIX_FMT_GBRP10BE,  ///< planar GBR 4:4:4 30bpp, big-endian
AV_PIX_FMT_GBRP10LE,  ///< planar GBR 4:4:4 30bpp, little-endian
AV_PIX_FMT_GBRP16BE,  ///< planar GBR 4:4:4 48bpp, big-endian
AV_PIX_FMT_GBRP16LE,  ///< planar GBR 4:4:4 48bpp, little-endian
....
```

除此之前像素还能用 YUV 表示，后续在学习使用 OpenGL 渲染视频时在介绍吧。

这个表怎么看，我以 AV_PIX_FMT_RGBA 举例说明
AV_PIX_FMT_RGBA,      ///< packed RGBA 8:8:8:8, 32bpp, RGBARGBA...

我们知道颜色可以由 RGB 混合而成，A 是透明度；RGBA 意味着使用R + G + B + A 四个分量来表示一个像素。

根据分量的排列又分为 planar 和 packed 两种形式，planar 排列的形式是把分量按类别分开，相同的类别放一起，所以 8 个像素的内存排列如下:

```
R:RRRRRRRR
G:GGGGGGGG
B:BBBBBBBB
A:AAAAAAAA
```
这四个分量与 AVFrame 的 data 指针数组一一对应。

同样是 8 个像素，使用 packed 排列形式内存如下:

```
RGBARGBARGBARGBARGBARGBARGBARGBA
```
这个分量与 AVFrame 的 data 指针数组第一个元素对应。

然后 8:8:8:8, 32bpp 的意思是，每个分量分别使用 8 个 bit，每个像素共计 32 个 bit。这么多的表示形式到底该用哪个呢？

在 [Quartz 2D Programming Guide](https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_context/dq_context.html#//apple_ref/doc/uid/TP30001066-CH203-TPXREF101) 里找到了一个表格

![Table 2-1  Pixel formats supported for bitmap graphics contexts
](./imgs/0x07/Pixel formats.png)

这个表格里的 bpp 已经说过了，表示每个像素使用多个 bit 表示。而 bpc 意思是每个组成分量使用多个 bit 表示，表格里 bpc 的值只有一个，意思是每个分量占用的 bit 相等，因此通过 bpc 可以过滤掉 FFmpeg 支持的 555、565 之类个格式了，加上 bpp 这个条件进一步过滤，剩下的就不太多了。看个例子吧，16 bpp 的只有一个，但是 bpc 是 5，那就意味着不支持 Alpha 通道了，但是 5 x 3 = 15 整个像素却使用 16 个 bit 表示，这要怎么理解呢？剩下那一个 bit 如何安置呢？通过表格看到了 kCGImageAlphaNoneSkipFirst 这个，意思是不支持 Alpha 并且跳过第一个字节！然后很容易能找到 FFmpeg 里与之对应的有两个：

```
AV_PIX_FMT_RGB555BE,  ///< packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), big-endian   , X=unused/undefined
AV_PIX_FMT_RGB555LE,  ///< packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), little-endian, X=unused/undefined
```
这两个的区别是 big-endian 和 little-endian，学习过网络编程的基本都听说过，这是字节排序的两种形式，跟 CPU 有关系，这里就不展开讨论了。

了解到 RGB 像素格式的 Bitmap 可以创建 CGImage，因此使用 Core Graphies 渲染视频帧的前提是如何将视频像素格式转换成 RGB！好在 FFmpeg 提供了  **libswscale** 库，这个库就是用来做视频像素格式转换，图像缩放的。

- 需要提供一个类封装下 libswcale 库提供的函数，负责像素格式的转换，在解码后直接调用即可；

- 需要提供一个工具类将 RGB 像素格式的 AVFrame 转换成 UIImage，封装下转换过程；
- 需要将转换好的 CGImage 传给调用者，这里选择代理模式；

## 核心代码

### 像素格式转换类：FFVideoScale0x07 

结合 libswscale 库的使用方式和参数，封装后提供了一个初始化方法，传递必要的参数：

```objc
/// @param srcPixFmt 原帧像素格式
/// @param dstPixFmt 目标帧像素格式
/// @param picWidth 图像宽度
/// @param picHeight 图像高度
- (instancetype)initWithSrcPixFmt:(int)srcPixFmt
                        dstPixFmt:(int)dstPixFmt
                         picWidth:(int)picWidth
                        picHeight:(int)picHeight;
```

需要转换时调用：

```objc
/// @param inF 需要转换的帧
/// @param outP 转换的结果[不要free相关内存，通过ref/unref的方式使用]
- (BOOL) rescaleFrame:(AVFrame *)inF out:(AVFrame *_Nonnull*_Nonnull)outP;
```

libswcale 库使用步骤：

```objc
//初始化
self.sws_ctx = sws_getContext(picWidth, picHeight, srcPixFmt, picWidth, picHeight, dstPixFmt, SWS_POINT, NULL, NULL, NULL);
//分配内存空间
self.frame = av_frame_alloc();
av_image_fill_linesizes(out_frame->linesize, out_frame->format, out_frame->width);
av_image_alloc(out_frame->data, out_frame->linesize, self.picWidth, self.picHeight, self.dstPixFmt, 1);
//转换
int ret = sws_scale(self.sws_ctx, (const uint8_t* const*)inF->data, inF->linesize, 0, inF->height, out_frame->data, out_frame->linesize);
```


### AVFrame 转 CGImage 工具类：MRConvertUtil

AVFrame 的像素格式必须是这些才可以调用这个类进行转换！

```
5R 5G 5B(lsb), big-endian   , X=unused/undefined
AV_PIX_FMT_RGB555LE     ///< packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), little-endian, X=unused/undefined
AV_PIX_FMT_0RGB,        ///< packed RGB 8:8:8, 32bpp, XRGBXRGB...   X=unused/undefined
AV_PIX_FMT_RGB0,        ///< packed RGB 8:8:8, 32bpp, RGBXRGBX...   X=unused/undefined
AV_PIX_FMT_RGBA,     ///< packed RGBA 8:8:8:8, 32bpp, RGBARGBA...
AV_PIX_FMT_ARGB,        ///< packed ARGB 8:8:8:8, 32bpp, ARGBARGB...
AV_PIX_FMT_RGB555BE,    ///< packed RGB 5:5:5, 16bpp, (msb)1X AV_PIX_FMT_RGB24,   ///< packed RGB 8:8:8, 24bpp, RGBRGB...
```

转成 CGImage 的代码如下：

```objc
CGImageRef _CreateCGImageFromBitMap(void *pixels,size_t w, size_t h,
size_t bpc, size_t bpp, size_t bpr, int bmi)
{
    assert(bpp != 24);
    /*
     AV_PIX_FMT_RGB24 bpp is 24! not supported!
     Crash:
     2020-06-06 00:08:20.245208+0800 FFmpegTutorial[23649:2335631] [Unknown process name] CGBitmapContextCreate: unsupported parameter combination: set CGBITMAP_CONTEXT_LOG_ERRORS environmental variable to see the details
     2020-06-06 00:08:20.245417+0800 FFmpegTutorial[23649:2335631] [Unknown process name] CGBitmapContextCreateImage: invalid context 0x0. If you want to see the backtrace, please set CG_CONTEXT_SHOW_BACKTRACE environmental variable.
     */
    
    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
    
    CGContextRef bitmapContext = CGBitmapContextCreate(
        pixels,
        w,
        h,
        bpc,
        bpr,
        colorSpace,
        bmi
    );
    
    CGImageRef cgImage = CGBitmapContextCreateImage(bitmapContext);
    CGColorSpaceRelease(colorSpace);
    return CFAutorelease(cgImage);
}

CGImageRef _CreateCGImage(void *pixels,size_t w, size_t h,
size_t bpc, size_t bpp, size_t bpr, int bmi)
{
    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
    
    const UInt8 *rgb = pixels;
    const CFIndex length = bpr * h;
    ///需要copy！因为frame是重复利用的；里面的数据会变化！
    CFDataRef data = CFDataCreate(kCFAllocatorDefault, rgb, length);
    CGDataProviderRef provider = CGDataProviderCreateWithCFData(data);
    CFRelease(data);
    
    CGImageRef cgImage = CGImageCreate(w,
                                       h,
                                       bpc,
                                       bpp,
                                       bpr,
                                       colorSpace,
                                       bmi,
                                       provider,
                                       NULL,
                                       NO,
                                       kCGRenderingIntentDefault);
    CGDataProviderRelease(provider);
    CGColorSpaceRelease(colorSpace);
    
    return CFAutorelease(cgImage);
}


+ (CGImageRef)cgImageFromRGBFrame:(AVFrame*)frame
{
    if (frame->format == AV_PIX_FMT_RGB555BE || frame->format == AV_PIX_FMT_RGB555LE || frame->format == AV_PIX_FMT_RGB24 || frame->format == AV_PIX_FMT_RGBA || frame->format == AV_PIX_FMT_0RGB || frame->format == AV_PIX_FMT_RGB0 || frame->format == AV_PIX_FMT_ARGB || frame->format == AV_PIX_FMT_RGBA) {
        //these are supported!
    } else {
        NSAssert(NO, @"not support [%d] Pixel format,use RGB555BE/RGB555LE/RGBA/ARGB/0RGB/RGB24 please!",frame->format);
    }
//    https://stackoverflow.com/questions/1579631/converting-rgb-data-into-a-bitmap-in-objective-c-cocoa
    //https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_context/dq_context.html#//apple_ref/doc/uid/TP30001066-CH203-BCIBHHBB
    
    int bpc = 0;
    int bpp = 0;
    CGBitmapInfo bitMapInfo = 0;
    if (frame->format == AV_PIX_FMT_RGB555BE) {
        bpc = 5;
        bpp = 16;
        bitMapInfo = kCGBitmapByteOrder16Big | kCGImageAlphaNoneSkipFirst;
    } else if (frame->format == AV_PIX_FMT_RGB555LE) {
        bpc = 5;
        bpp = 16;
        bitMapInfo = kCGBitmapByteOrder16Little | kCGImageAlphaNoneSkipFirst;
    } else if (frame->format == AV_PIX_FMT_RGB24) {
        bpc = 8;
        bpp = 24;
        bitMapInfo = kCGImageAlphaNone | kCGBitmapByteOrderDefault;
    } else if (frame->format == AV_PIX_FMT_0RGB) {
        //AV_PIX_FMT_0RGB 当做已经预乘好的 AV_PIX_FMT_ARGB 也可以渲染出来，总之不让 GPU 再次计算就行了
        bpc = 8;
        bpp = 32;
        bitMapInfo = kCGBitmapByteOrderDefault |kCGImageAlphaNoneSkipFirst;
    } else if (frame->format == AV_PIX_FMT_RGB0) {
       //AV_PIX_FMT_RGB0 当做已经预乘好的 AV_PIX_FMT_RGBA 也可以渲染出来，总之不让 GPU 再次计算就行了
       bpc = 8;
       bpp = 32;
       bitMapInfo = kCGBitmapByteOrderDefault |kCGImageAlphaNoneSkipLast;
    } else if (frame->format == AV_PIX_FMT_ARGB) {
        bpc = 8;
        bpp = 32;
        bitMapInfo = kCGBitmapByteOrderDefault | kCGImageAlphaPremultipliedFirst;
    } else if (frame->format == AV_PIX_FMT_RGBA) {
        bpc = 8;
        bpp = 32;
        ///已经预乘好的，不让GPU再次计算，直接渲染就行了
        bitMapInfo = kCGBitmapByteOrderDefault | kCGImageAlphaPremultipliedLast;
    } else {
        NSAssert(NO, @"WTF!");
    }
    void *pixels = frame->data[0];
    const size_t bpr = frame->linesize[0];
    const int w = frame->width;
    const int h = frame->height;
    
    return _CreateCGImage(pixels, w, h, bpc, bpp, bpr, bitMapInfo);
    //not support bpp = 24;
    return _CreateCGImageFromBitMap(pixels, w, h, bpc, bpp, bpr, bitMapInfo);
}
```

### 改造播放器

1、在创建视频解码对象之后，创建像素格式转换对象

```objc
...
self.videoDecoder.name = @"videoDecoder";
self.videoScale = [self createVideoScaleIfNeed];
...

- (FFVideoScale0x07 *)createVideoScaleIfNeed {
    //未指定期望像素格式
    if (self.supportedPixelFormats == MR_PIX_FMT_MASK_NONE) {
        NSAssert(NO, @"supportedPixelFormats can't be none!");
        return nil;
    }
    
    //当前视频的像素格式
    const enum AVPixelFormat format = self.videoDecoder.pix_fmt;
    
    bool matched = false;
    MRPixelFormat firstSupportedFmt = MR_PIX_FMT_NONE;
    for (int i = 0; i < sizeof(ALL_MR_PIX_FMTS)/sizeof(MRPixelFormat); i ++) {
        const MRPixelFormat fmt = ALL_MR_PIX_FMTS[i];
        const MRPixelFormatMask mask = 1 << fmt;
        if (self.supportedPixelFormats & mask) {
            if (firstSupportedFmt == MR_PIX_FMT_NONE) {
                firstSupportedFmt = fmt;
            }
            
            if (format == MRPixelFormat2AV(fmt)) {
                matched = true;
                break;
            }
        }
    }
    
    if (matched) {
        //期望像素格式包含了当前视频像素格式，则直接使用当前格式，不再转换。
        return nil;
    }
    
    if (firstSupportedFmt == MR_PIX_FMT_NONE) {
        NSAssert(NO, @"supportedPixelFormats is invalid!");
        return nil;
    }
    
    ///创建像素格式转换上下文
    FFVideoScale0x07 *scale = [[FFVideoScale0x07 alloc] initWithSrcPixFmt:format dstPixFmt:MRPixelFormat2AV(firstSupportedFmt) picWidth:self.videoDecoder.picWidth picHeight:self.videoDecoder.picHeight];
    return scale;
}

在接收到解码器解出的 frame 之后，进行转换：
...
else if (decoder == self.videoDecoder) {
  FrameQueue *fq = &pictq;

  AVFrame *outP = nil;
  if (self.videoScale) {
    if (![self.videoScale rescaleFrame:frame out:&outP]) {
      #warning TODO handle sacle error
    }
  } else {
    outP = frame;
  }

  Frame *af = NULL;
  if (NULL != (af = frame_queue_peek_writable(fq))) {
    av_frame_ref(af->frame, outP);
    frame_queue_push(fq);
  }
}
...
```

2、渲染线程转成UIImage，并代理出去

```objc
//定义代理协议
@protocol FFPlayer0x07Delegate <NSObject>

@optional
- (void)reveiveFrameToRenderer:(CGImageRef *)img;

@end
 
//通知委托者
if ([self.delegate respondsToSelector:@selector(reveiveFrameToRenderer:)]) {
  @autoreleasepool {
        CGImageRef img = [MRConvertUtil cgImageFromRGBFrame:vp->frame];
        [self.delegate reveiveFrameToRenderer:img];
   }
}
```

3、播放器的调用

```objc
//指定支持的像素格式，播放器内部就会自动转换了
player.supportedPixelFormats = MR_PIX_FMT_MASK_RGB0;
player.delegate = self;

//代理方法会在渲染线程通知有新的 CGImage 需要渲染
- (void)reveiveFrameToRenderer:(CGImageRef)cgImage
{
    CFRetain(cgImage);
    dispatch_async(dispatch_get_main_queue(), ^{
        [self.videoRenderer dispalyCGImage:cgImage];
        CFRelease(cgImage);
    });
}
```


## 总结

值得一说的是，FFVideoScale0x07 的 **输入和输出类型一致**，不会提高对后续接口的对接难度，因为当原帧像素格式跟目标帧像素格式一样时，就不用转换了，这时后续接口是直接使用 AVFrame 的！其内部实现也同样精彩，为避免多次申请内存空间，转换过程仿造解码过程，复用了一个 AVFrame 对象，提高了内存使用效率！外部调用时完全遵照 AVFrame 引用计数的正常使用方式即可！

结合掩码设计，提供了几种常见的像素格式，在调用播放器的时候通过 supportedPixelFormats 指定即可，如果指定了多个，内部会优先匹配，如果不能匹配则自动选择其中一种进行格式转换！这一设计为后续篇章使用其他方法渲染做个铺垫，其他的渲染方式，需要的是别的像素格式，具体是哪些格式，请接着往后看吧！