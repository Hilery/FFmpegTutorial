# 播放器总体架构设计



今天来聊一聊第一阶段的第一大问题，需要写的逻辑那么多，从哪里入手呢？麻雀虽小，五脏俱全，需要先对播放做一个总体架构的设计，然后按照这个设计把逻辑拆分成模块去逐一攻克，下面是播放器播放视频的流程图：

```flow
st=>start: 视频文件(http,file,rtmp...)
protocol=>inputoutput: 解协议
container=>operation: 封装格式数据
demux=>inputoutput: 解封装
spec_av=>parallel: 分离音视频
audio=>operation: 音频码流包
video=>operation: 视频码流包
audio_dec=>inputoutput: 音频解码
video_dec=>inputoutput: 视频解码
audio_frame=>operation: 音频帧数据
video_frame=>operation: 视频帧数据
av_sync=>inputoutput: 音视频同步播放
av_sync2=>inputoutput: 音视频同步播放
audio_dis=>operation: 音频🔈
video_dis=>operation: 视频🖥
ed=>end: 结束

st->protocol->container->demux->spec_av
spec_av(path1,right)->audio->audio_dec->audio_frame->av_sync->audio_dis->ed
spec_av(path2,left)->video->video_dec->video_frame->av_sync2->video_dis->ed

```



1、解协议：将要播放的视频文件（流）按照协议格式，获取到封装格式的数据，常见的协议包括：http(s),file,rtmp等，通俗的讲就是按照协议去下载文件。

2、解封装：视频文件是有[封装格式](./illiteracy/0x01.md)的，因此需要解封装，这个过程一般称为 demux，与之相对的封装视频称之为 mux。经过 demux 之后可以得到音视频字幕等流数据，根据媒体类型分为不同的 Stream，通过 Stream 可以进一步获取到音视频相关的更多信息，大多数情况下拿到的是 H264 编码格式的视频流和 ACC编码格式的音频流。

在 FFmpeg 里解码前或编码后的数据使用 AVPacket 结构体存放，码流数据以包的形式存在，一个 packet 就是一个包，这些包将通过后续的解码函数解码，这是一个生产者与消费者的关系，为了保证提供充足的 packet ，因此需要设计一个 packet 缓存队列，因为 packet 里存放的是压缩数据，因此每个包内存占用并不大，可以把 buffer 设置的大一点，同时根据时长等条件约束，具体可看 0x03 的代码实现。

3、解码：解封之后的 AVPacket 包是被压缩的，因为原始的视频画面和音频数据量太大了，因此在视频制作时使用不同的编码方式，对音视频做了压缩。在 FFmpeg 里调用解码函数解码后的数据放在 AVFrame 里，称之为一帧。一个视频 packet 可以解出来一帧视频 frame，一个音频 packet 可以解出来一帧或者多帧音频 frame。

视频 packet 解码之后得到的 frame 里面装的是什么？是一个一个的像素，像素也是有格式的，最常用的格式是 YUV 和 RGB。有了像素级别的数据，再加上图像宽高就可以跟 OpenGL 等视频渲染层对接了，后续篇章会介绍几种渲染的方式。

音频 packet 解码之后得到的 frame 里面装的是什么？是一个一个的采样，采样有不同的精度和频率，最常见的精度是 S16,Float,S16P,FloatP。有了这些采样数据，再加上采样率等参数就可以交给诸如 AudioUnit 之类的API去播放声音了，这个后续篇章也会详细介绍。

程序需要设计一个 frame 缓存队列，提前缓存一部分 frame ，供渲染使用，防止出现频繁卡顿。不妨看看音视频帧的播放速度吧，视频画面播放的本质是快速播放一系列连续动作的图像，有多快？最少每秒播放24副图像，否者就会看起来动作不连贯，有卡顿。音频每秒播放的采样点要多 2 个数量级了，拿最常见的 44.1KHZ 的采样率来说每秒要播放 44100个采样，一个音频帧包含多少个样呢？跟压缩编码有关系，比如 AAC 帧一帧包含 1024 个采样点，MP3 则包含 1152 个采样点。 所以对于常见的 25 帧 44.1KHZ 采用AAC编码视频来说每秒需要消耗掉 43 个音频帧和 24 个视频帧。解码后的数据量大，所以 buffer 不能过小也不能过大了。太小的话，渲染时可能会出现 buffer 空的情况，太大了可能造成内存吃紧甚至 OOM。由于音视频分开解码，并且视频帧比音频帧大，所以会分配两个 frame 缓存队列，各自为政。

4、音视频同步：

播放器的好坏很大程度取决于音视频的同步，为什么这么说呢？因为即使深度优化解码器提升效率以节省 CPU 资源、优化网络协议以节省带宽，解码数据设计优良的内存循环管理系统，最终的目的是呈现给用户高清的画面，和悦耳的声音，让用户好好享受视频带来的欢乐！可是如果一旦声音和画面出现了明显的不同步，用户察觉到了之后，观感体验就会大幅下降，甚至于关闭视频，这是我们最不愿意看到的结果了。

同步策略无非就三种，音频向视频同步，视频向音频同步，音视频向外部时钟同步。

- 音频向视频同步：以视频为主的同步方式，当音频落后于视频时就加快或丢弃音频帧从而达到追赶的目的，反之，则减慢或重复音频帧从而达到等待视频的目的。

- 视频向音频同步：以音频为主的同步方式，当视频落后于音频时就加快或丢弃视频帧从而达到追赶的目的，反之，则减慢或重复视频帧从而达到等待音频的目的。

- 音视频向外部主时钟同步：以外部时钟为主的同步方式，当音/视频落后于主时钟时就加快或丢弃音/视频帧从而达到追赶的目的，反之，则减慢或重复音/视频帧从而达到等待音频的目的。

应该使用哪种方式呢？这个要从生物学角度出发，根据人对声音比画面更加敏感这一事实来选择，可以先排除掉音频向视频同步，因为音频的加快或减慢都是很容易被察觉到的，跟别说是丢弃或重复了。以外部时钟为主的同步方式也不可取，虽然这个策略，可以做到比另外两种都要精确的同步，但是由于音视频都需要作出妥协，体验上并不好，很容易被察觉到；因此选择视频向音频同步是最为合适的，保证音频的播放，视频适当的加快或放慢，一般是看不出画面有停滞或跳跃的。

后续代码将会大致按照读包，解码，渲染，同步的步骤逐一分析实现。