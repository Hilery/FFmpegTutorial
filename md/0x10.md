
## 0x10 使用 Core Graphies 渲染视频帧

从本篇教程开始介绍 iOS 平台的视频渲染技术，我至少介绍 5 种渲染视频的方法，并且对比他们的性能。

本篇教程在  [0x06](./0x06.md) 教程的基础之上增加了两个功能：

- 视频像素格式转换
- 使用 Core Graphies 渲染视频帧

## 思路分析

若要使用 Core Graphies API 去渲染视频帧，那么必须要将 AVFrame 转成 CGImage 才行，了解到 RGB 像素格式的 Bitmap 可以创建 CGImage，因此使用 Core Graphies 渲染视频帧的前提是如何将视频帧像素格式转换成 RGB！好在 FFmpeg 提供了  **libswscale** 库，这个库就是用来做视频像素格式的转换以及图像缩放的。

在 <libavutil/pixfmt.h> 头文件里定义了 FFmpeg 支持的所有像素格式，我只把 RGB 相关的贴出来一起看下：

```
AV_PIX_FMT_RGB24,     ///< packed RGB 8:8:8, 24bpp, RGBRGB...
AV_PIX_FMT_BGR24,     ///< packed RGB 8:8:8, 24bpp, BGRBGR...
AV_PIX_FMT_BGR8,      ///< packed RGB 3:3:2,  8bpp, (msb)2B 3G 3R(lsb)
AV_PIX_FMT_BGR4,      ///< packed RGB 1:2:1 bitstream,  4bpp, (msb)1B 2G 1R(lsb), a byte contains two pixels, the first pixel in the byte is the one composed by the 4 msb bits
AV_PIX_FMT_BGR4_BYTE, ///< packed RGB 1:2:1,  8bpp, (msb)1B 2G 1R(lsb)
AV_PIX_FMT_RGB8,      ///< packed RGB 3:3:2,  8bpp, (msb)2R 3G 3B(lsb)
AV_PIX_FMT_RGB4,      ///< packed RGB 1:2:1 bitstream,  4bpp, (msb)1R 2G 1B(lsb), a byte contains two pixels, the first pixel in the byte is the one composed by the 4 msb bits
AV_PIX_FMT_RGB4_BYTE, ///< packed RGB 1:2:1,  8bpp, (msb)1R 2G 1B(lsb)
AV_PIX_FMT_ARGB,      ///< packed ARGB 8:8:8:8, 32bpp, ARGBARGB...
AV_PIX_FMT_RGBA,      ///< packed RGBA 8:8:8:8, 32bpp, RGBARGBA...
AV_PIX_FMT_ABGR,      ///< packed ABGR 8:8:8:8, 32bpp, ABGRABGR...
AV_PIX_FMT_BGRA,      ///< packed BGRA 8:8:8:8, 32bpp, BGRABGRA...
AV_PIX_FMT_RGB48BE,   ///< packed RGB 16:16:16, 48bpp, 16R, 16G, 16B, the 2-byte value for each R/G/B component is stored as big-endian
AV_PIX_FMT_RGB48LE,   ///< packed RGB 16:16:16, 48bpp, 16R, 16G, 16B, the 2-byte value for each R/G/B component is stored as little-endian
AV_PIX_FMT_RGB565BE,  ///< packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), big-endian
AV_PIX_FMT_RGB565LE,  ///< packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), little-endian
AV_PIX_FMT_RGB555BE,  ///< packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), big-endian   , X=unused/undefined
AV_PIX_FMT_RGB555LE,  ///< packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), little-endian, X=unused/undefined
AV_PIX_FMT_BGR565BE,  ///< packed BGR 5:6:5, 16bpp, (msb)   5B 6G 5R(lsb), big-endian
AV_PIX_FMT_BGR565LE,  ///< packed BGR 5:6:5, 16bpp, (msb)   5B 6G 5R(lsb), little-endian
AV_PIX_FMT_BGR555BE,  ///< packed BGR 5:5:5, 16bpp, (msb)1X 5B 5G 5R(lsb), big-endian   , X=unused/undefined
AV_PIX_FMT_BGR555LE,  ///< packed BGR 5:5:5, 16bpp, (msb)1X 5B 5G 5R(lsb), little-endian, X=unused/undefined
AV_PIX_FMT_RGB444LE,  ///< packed RGB 4:4:4, 16bpp, (msb)4X 4R 4G 4B(lsb), little-endian, X=unused/undefined
AV_PIX_FMT_RGB444BE,  ///< packed RGB 4:4:4, 16bpp, (msb)4X 4R 4G 4B(lsb), big-endian,    X=unused/undefined
AV_PIX_FMT_BGR444LE,  ///< packed BGR 4:4:4, 16bpp, (msb)4X 4B 4G 4R(lsb), little-endian, X=unused/undefined
AV_PIX_FMT_BGR444BE,  ///< packed BGR 4:4:4, 16bpp, (msb)4X 4B 4G 4R(lsb), big-endian,    X=unused/undefined
AV_PIX_FMT_BGR48BE,   ///< packed RGB 16:16:16, 48bpp, 16B, 16G, 16R, the 2-byte value for each R/G/B component is stored as big-endian
AV_PIX_FMT_BGR48LE,   ///< packed RGB 16:16:16, 48bpp, 16B, 16G, 16R, the 2-byte value for each R/G/B component is stored as little-endian
AV_PIX_FMT_GBRP,      ///< planar GBR 4:4:4 24bpp
AV_PIX_FMT_GBR24P = AV_PIX_FMT_GBRP, // alias for #AV_PIX_FMT_GBRP
AV_PIX_FMT_GBRP9BE,   ///< planar GBR 4:4:4 27bpp, big-endian
AV_PIX_FMT_GBRP9LE,   ///< planar GBR 4:4:4 27bpp, little-endian
AV_PIX_FMT_GBRP10BE,  ///< planar GBR 4:4:4 30bpp, big-endian
AV_PIX_FMT_GBRP10LE,  ///< planar GBR 4:4:4 30bpp, little-endian
AV_PIX_FMT_GBRP16BE,  ///< planar GBR 4:4:4 48bpp, big-endian
AV_PIX_FMT_GBRP16LE,  ///< planar GBR 4:4:4 48bpp, little-endian
....
```

除此之外像素还能用 YUV 表示，后续介绍其他渲染方式时会用到，到时再介绍吧。

该如何理解这些像素格式？有什么区别？我以 AV_PIX_FMT_RGBA 举例说明
AV_PIX_FMT_RGBA,      ///< packed RGBA 8:8:8:8, 32bpp, RGBARGBA...

我们知道颜色可以由 RGB 混合而成，其中 A 是透明度；RGBA 意味着使用 R + G + B + A 四个分量来表示一个像素的颜色。根据分量的排列又分为 planar 和 packed 两种形式，planar 排列的形式是把分量按类别分开，相同的分量放一起，所以 8 个 RGBA 格式的像素内存排列如下:

```
R:RRRRRRRR
G:GGGGGGGG
B:BBBBBBBB
A:AAAAAAAA
```
这四个分量使用指针数组表示，也就是 AVFrame 的 data 成员变量，因此想访问分量数据使用 frame->data[i] 即可。

同样是 8 个像素，下面则是使用 packed 排列形式的内存排列:

```
RGBARGBARGBARGBARGBARGBARGBARGBA
```
这个分量可通过 frame->data[0] 来访问。

然后 8:8:8:8, 32bpp 的意思是：每个分量分别使用 8 个 bit，每个像素共计 32 个 bit，可以看出 bpp（每个像素使用多个 bit ） 小的话，占用的空间就会少些。

FFmpeg 支持了这么多的像素格式，使用 Core Grapyics 渲染时到底该用哪个呢？在 [Quartz 2D Programming Guide](https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_context/dq_context.html#//apple_ref/doc/uid/TP30001066-CH203-TPXREF101) 里我找到了下面这个表格：

![Table 2-1  Pixel formats supported for bitmap graphics contexts
](./imgs/0x07/Pixel formats.png)

表格里除了提到了 bpp （每个像素使用多个 bit ）也提到了 bpc （每个组成分量使用多个 bit 表示），并且 bpc 的值只有一个，意味着 **所有分量占用的 bit 数相等** 。表里没提到 packed 或者 planar，是因为不支持 planar 形式的，只支持 packed 形式的。

以 16 bpp，5bpc，kCGImageAlphaNoneSkipFirst 举例说明 ，该格式每个像素使用 16 个 bit 来表示，每个分量使用  5 个 bit 表示，一共 16个，这里用了15个，剩下那一个 bit 如何安置呢？SkipFirst！意思是跳过第一个字节！并且不支持 Alpha 通道，搞懂了这些布局信息后，很容易就能从 FFmpeg 里找到与之对应的像素格式了：

```
AV_PIX_FMT_RGB555BE,  ///< packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), big-endian   , X=unused/undefined
AV_PIX_FMT_RGB555LE,  ///< packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), little-endian, X=unused/undefined
```
这两个的区别是 big-endian 和 little-endian，学习过网络编程的应该都听说过才对，这是字节排序的两种形式，跟 CPU 有关系，这里就不展开讨论了。

iOS 还支持了 4 种 32 bpp, 8 bpc 的格式，他们与 FFmpeg 里的 AV_PIX_FMT_0RGB，AV_PIX_FMT_RGB0，AV_PIX_FMT_ARGB，AV_PIX_FMT_RGBA 对应，这里就不再一一分析了，给聪明的你留个作业。

最后下面这个清单是我们需要做的：

- 需要提供一个类封装下 libswcale 库提供的函数，负责像素格式的转换，在解码后直接调用即可；

- 需要提供一个工具类将 RGB 像素格式的 AVFrame 转换成 CGImage，封装下转换过程；
- 需要将转换好的 CGImage 传给调用者进行渲染，使用代理模式回调；

## 核心代码

### 像素格式转换类：FFVideoScale0x07 

结合 libswscale 库的使用方式和参数，封装后提供了一个初始化方法，传递必要的参数：

```objc
/// @param srcPixFmt 原帧像素格式
/// @param dstPixFmt 目标帧像素格式
/// @param picWidth  图像宽度
/// @param picHeight 图像高度
- (instancetype)initWithSrcPixFmt:(int)srcPixFmt
                        dstPixFmt:(int)dstPixFmt
                         picWidth:(int)picWidth
                        picHeight:(int)picHeight;
```

需要转换时调用：

```objc
/// @param inF  需要转换的帧
/// @param outP 转换的结果[不要free相关内存，通过ref/unref的方式使用]
- (BOOL) rescaleFrame:(AVFrame *)inF out:(AVFrame *_Nonnull*_Nonnull)outP;
```

libswcale 库使用步骤：

```objc
//初始化
self.sws_ctx = sws_getContext(picWidth, picHeight, srcPixFmt, picWidth, picHeight, dstPixFmt, SWS_POINT, NULL, NULL, NULL);
//分配内存空间
self.frame = av_frame_alloc();
av_image_fill_linesizes(out_frame->linesize, out_frame->format, out_frame->width);
av_image_alloc(out_frame->data, out_frame->linesize, self.picWidth, self.picHeight, self.dstPixFmt, 1);
//转换
int ret = sws_scale(self.sws_ctx, (const uint8_t* const*)inF->data, inF->linesize, 0, inF->height, out_frame->data, out_frame->linesize);
```


### AVFrame 转 CGImage 工具类：MRConvertUtil

AVFrame 的像素格式必须是这些才可以调用这个类进行转换！

```
AV_PIX_FMT_RGB555LE     ///< packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), little-endian, X=unused/undefined
AV_PIX_FMT_RGB555BE,    ///< packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), big-endian   , X=unused/undefined
AV_PIX_FMT_0RGB,        ///< packed RGB 8:8:8, 32bpp, XRGBXRGB...   X=unused/undefined
AV_PIX_FMT_RGB0,        ///< packed RGB 8:8:8, 32bpp, RGBXRGBX...   X=unused/undefined
AV_PIX_FMT_RGBA,     		///< packed RGBA 8:8:8:8, 32bpp, RGBARGBA...
AV_PIX_FMT_ARGB,        ///< packed ARGB 8:8:8:8, 32bpp, ARGBARGB...
AV_PIX_FMT_RGB24,       ///< packed RGB 8:8:8, 24bpp, RGBRGB...
```

转成 CGImage 的代码如下：

```objc
CGImageRef _CreateCGImageFromBitMap(void *pixels,size_t w, size_t h,
size_t bpc, size_t bpp, size_t bpr, int bmi)
{
    assert(bpp != 24);
    /*
     AV_PIX_FMT_RGB24 bpp is 24! not supported!
     Crash:
     2020-06-06 00:08:20.245208+0800 FFmpegTutorial[23649:2335631] [Unknown process name] CGBitmapContextCreate: unsupported parameter combination: set CGBITMAP_CONTEXT_LOG_ERRORS environmental variable to see the details
     2020-06-06 00:08:20.245417+0800 FFmpegTutorial[23649:2335631] [Unknown process name] CGBitmapContextCreateImage: invalid context 0x0. If you want to see the backtrace, please set CG_CONTEXT_SHOW_BACKTRACE environmental variable.
     */
    
    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
    
    CGContextRef bitmapContext = CGBitmapContextCreate(
        pixels,
        w,
        h,
        bpc,
        bpr,
        colorSpace,
        bmi
    );
    
    CGImageRef cgImage = CGBitmapContextCreateImage(bitmapContext);
    CGColorSpaceRelease(colorSpace);
    return CFAutorelease(cgImage);
}

CGImageRef _CreateCGImage(void *pixels,size_t w, size_t h,
size_t bpc, size_t bpp, size_t bpr, int bmi)
{
    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
    
    const UInt8 *rgb = pixels;
    const CFIndex length = bpr * h;
    ///需要copy！因为frame是重复利用的；里面的数据会变化！
    CFDataRef data = CFDataCreate(kCFAllocatorDefault, rgb, length);
    CGDataProviderRef provider = CGDataProviderCreateWithCFData(data);
    CFRelease(data);
    
    CGImageRef cgImage = CGImageCreate(w,
                                       h,
                                       bpc,
                                       bpp,
                                       bpr,
                                       colorSpace,
                                       bmi,
                                       provider,
                                       NULL,
                                       NO,
                                       kCGRenderingIntentDefault);
    CGDataProviderRelease(provider);
    CGColorSpaceRelease(colorSpace);
    
    return CFAutorelease(cgImage);
}


+ (CGImageRef)cgImageFromRGBFrame:(AVFrame*)frame
{
    if (frame->format == AV_PIX_FMT_RGB555BE || frame->format == AV_PIX_FMT_RGB555LE || frame->format == AV_PIX_FMT_RGB24 || frame->format == AV_PIX_FMT_RGBA || frame->format == AV_PIX_FMT_0RGB || frame->format == AV_PIX_FMT_RGB0 || frame->format == AV_PIX_FMT_ARGB || frame->format == AV_PIX_FMT_RGBA) {
        //these are supported!
    } else {
        NSAssert(NO, @"not support [%d] Pixel format,use RGB555BE/RGB555LE/RGBA/ARGB/0RGB/RGB24 please!",frame->format);
    }
//    https://stackoverflow.com/questions/1579631/converting-rgb-data-into-a-bitmap-in-objective-c-cocoa
    //https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq_context/dq_context.html#//apple_ref/doc/uid/TP30001066-CH203-BCIBHHBB
    
    int bpc = 0;
    int bpp = 0;
    CGBitmapInfo bitMapInfo = 0;
    if (frame->format == AV_PIX_FMT_RGB555BE) {
        bpc = 5;
        bpp = 16;
        bitMapInfo = kCGBitmapByteOrder16Big | kCGImageAlphaNoneSkipFirst;
    } else if (frame->format == AV_PIX_FMT_RGB555LE) {
        bpc = 5;
        bpp = 16;
        bitMapInfo = kCGBitmapByteOrder16Little | kCGImageAlphaNoneSkipFirst;
    } else if (frame->format == AV_PIX_FMT_RGB24) {
        bpc = 8;
        bpp = 24;
        bitMapInfo = kCGImageAlphaNone | kCGBitmapByteOrderDefault;
    } else if (frame->format == AV_PIX_FMT_0RGB) {
        //AV_PIX_FMT_0RGB 当做已经预乘好的 AV_PIX_FMT_ARGB 也可以渲染出来，总之不让 GPU 再次计算就行了
        bpc = 8;
        bpp = 32;
        bitMapInfo = kCGBitmapByteOrderDefault |kCGImageAlphaNoneSkipFirst;
    } else if (frame->format == AV_PIX_FMT_RGB0) {
       //AV_PIX_FMT_RGB0 当做已经预乘好的 AV_PIX_FMT_RGBA 也可以渲染出来，总之不让 GPU 再次计算就行了
       bpc = 8;
       bpp = 32;
       bitMapInfo = kCGBitmapByteOrderDefault |kCGImageAlphaNoneSkipLast;
    } else if (frame->format == AV_PIX_FMT_ARGB) {
        bpc = 8;
        bpp = 32;
        bitMapInfo = kCGBitmapByteOrderDefault | kCGImageAlphaPremultipliedFirst;
    } else if (frame->format == AV_PIX_FMT_RGBA) {
        bpc = 8;
        bpp = 32;
        ///已经预乘好的，不让GPU再次计算，直接渲染就行了
        bitMapInfo = kCGBitmapByteOrderDefault | kCGImageAlphaPremultipliedLast;
    } else {
        NSAssert(NO, @"WTF!");
    }
    void *pixels = frame->data[0];
    const size_t bpr = frame->linesize[0];
    const int w = frame->width;
    const int h = frame->height;
    
    return _CreateCGImage(pixels, w, h, bpc, bpp, bpr, bitMapInfo);
    //not support bpp = 24;
    return _CreateCGImageFromBitMap(pixels, w, h, bpc, bpp, bpr, bitMapInfo);
}
```

### 改造播放器

1、在创建视频解码对象之后，创建像素格式转换对象

```objc
...
self.videoDecoder.name = @"videoDecoder";
self.videoScale = [self createVideoScaleIfNeed];
...

- (FFVideoScale0x07 *)createVideoScaleIfNeed {
    //未指定期望像素格式
    if (self.supportedPixelFormats == MR_PIX_FMT_MASK_NONE) {
        NSAssert(NO, @"supportedPixelFormats can't be none!");
        return nil;
    }
    
    //当前视频的像素格式
    const enum AVPixelFormat format = self.videoDecoder.pix_fmt;
    
    bool matched = false;
    MRPixelFormat firstSupportedFmt = MR_PIX_FMT_NONE;
    for (int i = 0; i < sizeof(ALL_MR_PIX_FMTS)/sizeof(MRPixelFormat); i ++) {
        const MRPixelFormat fmt = ALL_MR_PIX_FMTS[i];
        const MRPixelFormatMask mask = 1 << fmt;
        if (self.supportedPixelFormats & mask) {
            if (firstSupportedFmt == MR_PIX_FMT_NONE) {
                firstSupportedFmt = fmt;
            }
            
            if (format == MRPixelFormat2AV(fmt)) {
                matched = true;
                break;
            }
        }
    }
    
    if (matched) {
        //期望像素格式包含了当前视频像素格式，则直接使用当前格式，不再转换。
        return nil;
    }
    
    if (firstSupportedFmt == MR_PIX_FMT_NONE) {
        NSAssert(NO, @"supportedPixelFormats is invalid!");
        return nil;
    }
    
    ///创建像素格式转换上下文
    FFVideoScale0x07 *scale = [[FFVideoScale0x07 alloc] initWithSrcPixFmt:format dstPixFmt:MRPixelFormat2AV(firstSupportedFmt) picWidth:self.videoDecoder.picWidth picHeight:self.videoDecoder.picHeight];
    return scale;
}

在接收到解码器解出的 frame 之后，进行转换：
...
else if (decoder == self.videoDecoder) {
  FrameQueue *fq = &pictq;

  AVFrame *outP = nil;
  if (self.videoScale) {
    if (![self.videoScale rescaleFrame:frame out:&outP]) {
      #warning TODO handle sacle error
    }
  } else {
    outP = frame;
  }

  Frame *af = NULL;
  if (NULL != (af = frame_queue_peek_writable(fq))) {
    av_frame_ref(af->frame, outP);
    frame_queue_push(fq);
  }
}
...
```

2、渲染线程转成 CGImage，并代理出去

```objc
//定义代理协议
@protocol FFPlayer0x07Delegate <NSObject>

@optional
- (void)reveiveFrameToRenderer:(CGImageRef *)img;

@end
 
//通知委托者
if ([self.delegate respondsToSelector:@selector(reveiveFrameToRenderer:)]) {
  @autoreleasepool {
        CGImageRef img = [MRConvertUtil cgImageFromRGBFrame:vp->frame];
        [self.delegate reveiveFrameToRenderer:img];
   }
}
```

3、播放器的调用

```objc
//指定支持的像素格式，播放器内部就会自动转换了
player.supportedPixelFormats = MR_PIX_FMT_MASK_RGB0;
player.delegate = self;

//代理方法会在渲染线程通知有新的 CGImage 需要渲染
- (void)reveiveFrameToRenderer:(CGImageRef)cgImage
{
    CFRetain(cgImage);
    dispatch_async(dispatch_get_main_queue(), ^{
        [self.videoRenderer dispalyCGImage:cgImage];
        CFRelease(cgImage);
    });
}
```


## 总结

当解码之后的帧像素格式跟目标帧像素格式一样时不需要转换，否则调用 FFVideoScale0x07 进行转换。FFVideoScale0x07 转换的 **输入和输出均为 AVFrame **，这样做不会提高对后续接口的对接难度，这个是很多播放器没有考虑到的地方！其内部为避免频繁申请释放内存空间，转换过程仿造解码过程，复用了一个 AVFrame 对象，提高了内存使用效率！外部调用时完全遵照 AVFrame 引用计数的正常使用方式即可！

在调用播放器的时候通过 supportedPixelFormats 指定解码后的像素格式，并且通过掩码支持指定多种格式，内部会优先自动匹配，如果不能匹配则自动选择其中一种进行格式转换！这一设计为后续篇章使用其他方法渲染做个铺垫，其他的渲染方式，需要的就是别的像素格式了，具体是哪些格式，请接着往后看吧！

通过表格 2-1 可以确定 iOS 支持了 6 种像素格式，但实际调研发现有两种方法可以创建 CGImage，并且 CGImageCreate 支持 bpc ，比 CGBitmapContextCreate 更加灵活，因此可以支持表格上没有提到的 RGB24 格式，所以 **iOS 支持 7 种 RGB 像素格式**！

## iPhone 7 plus 真机实测数据

以下数据是以 Xcode 提供的数据为标准，并且不开启 Performance 监控（MRAppDelegate 里注释掉 [PerformanceWrapper show]; ），使用 demo 里提供的带时间水印的视频，像素格式使用 MR_PIX_FMT_MASK_0RGB 测量而来。

停留在首页时记录下数据：CPU 占用 0%，内存 17.3M；

进入 0x07ViewController 之后，观察渲染情况；

 - 第 20s 视频： CPU 占用 55%，内存 48.3M；
 - 第 40s 视频： CPU 占用 55%，内存 48.5M；
 - 第 61s 视频： CPU 占用 3%，内存 34.4M；

整个过程的峰值：

CPU 最高 56%， 内存最高 49.3M；

从数据来看，**使用 Core Graphics 渲染视频比较消耗 CPU 和内存**，不适合实际项目中使用！剩下 6 种像素格式，不再一一测试，比如换成 bpp 小的 MR_PIX_FMT_MASK_RGB555 格式占用内存也会略少，可自行测试。

内存占用为何如此之高？下面进行分析：

### 后备存储器 （backing store）

 WWDC 2012 Said: 

- Every UIView is backed by a CALayer 
- View layout is really layer layout
- -drawRect: draws into a CALayer backing store
- Layer properties and animations handled by render server
- Changes happen in CA::Transaction::commit()

为了搞清楚原因，我们需要弄明白期间都发生了什么。使用 Core Graphics 时需要先获取绘制需要的上下文，然后才能调用 Core Graphics 的相关 C 方法画图，而 UIView 的 \- (void)drawRect:(CGRect)rect 方法里提供好了这一环境，只需要去覆盖这个方法就行了，系统在调用 drawRect 之前会创建好这个上下文环境，只需要在这个方法里完成绘制即可，并且不要主动调用这个方法，系统会自动调用，需要重绘时，只需要调用 setNeedsDisplay ，听起来很美好不是么，但是 UIView 创建上下文需要付出代价，并且并不能预知后续会画什么图形，画到那个位置，所以系统会创建一个后备存储器 （backing store），用户存储自定义绘制的像素内容！并且该存储器占用的内存大小和当前 view 的尺寸成正比（跟 bpp 有关系；iOS 12 上优化后，不一定是正比的）。



获取绘制上下文有以下几种方式：

- 继承 UIView ，并覆盖 -drawRect: 方法，在 View 将要显示之前，系统会调用这个方法，并创建好上下文
- 继承 CALayer，并覆盖 -drawInContext: 方法，该方法的参数即是上下文，当调用 -setNeedsDisplay 后，会标记 display 需要调用，display 则会调用 -drawInContext: 方法。
- 成为 CALayer 的代理，实现 -drawLayer: inContext: 方法，这个方法会被默认的 -drawInContext 方法调用。
- 使用 UIKit 提供的便利方法：UIGraphicsBeginImageContext(CGSize size); 或者 UIGraphicsBeginImageContextWithOptions(CGSize size, BOOL opaque, CGFloat scale); 这两个方法使用Device 8bit RGB 色域，从 iOS 10 开始应该使用支持 16 bit 的 UIGraphicsImageRenderer ，以便支持更广色域！

### 绘制过程

在 drawRect 方法里调用的绘制指令最终由 CPU 在主线程中执行，并将生成的像素数据存储到后备存储器中，在适当的时候把后备存储器中的像素数据拷贝到帧缓冲区（GPU Frame Buffer），供GPU下次渲染使用。

### 渲染过程

iOS 设备的 GPU 使用了双缓冲技术，也就是说一个缓冲区用于当前屏幕显示，程序只需要把将要显示的内容放到另一个缓冲区里，等待 GPU 的 V-Sync 信号到来后，将两个缓冲区进行交互，即可显示新的缓冲区内容，如果 V-Sync 信号到来后，缓冲区里的内容还没来得及准备好，那么将显示上上一次的内容，这就是粘滞卡顿产生的原因。大部分设备的 V-Sync 信号频率是 60 HZ，部分设备可达 120HZ。

通过上面的分析可以看出三个问题：

- 绘制过程是由 CPU 在主线程完成的
- 后备存储器占用内存比实际画面要大
- 每一次重绘都会重新创建后备存储器重新绘制，没有复用

苹果不建议重写 drawRect 除非你有不可逾越的障碍，否则应当使用经过经过优化的UIKit 控件！在本例中由于我们使用 Core Graphics 绘制的是已经解码的 CGImage，所以 CPU 需要将 CGImage 的像素数据复制到指定区域对应的后备存储器里，可以理解为后备存储器里的数据包含了 CGImage，因为视频画面渲染时基本都会保持比例，然后保持比例后的画面往往不能够填充满整个设备屏幕，存在上下或者左右留白的情况！



## Instruments 分析

```
Target Name: MattReach

Target Model: iPhone 7 Plus (Model 1661, 1784, 1785, 1786)

Target iOS: 13.4.1 (17E262)

Host Name: xuqianlong's mbp

Host Model: MacBook Pro

Host macOS: 10.15.5 (19F101)

Template Name: Core Animation

Start Time: Jun 23, 2020 at 5:17:17 PM

End Time: Jun 23, 2020 at 5:18:47 PM

Duration: 1 minute, 30 seconds

End reason: User pressed Stop

Instruments: 11.5 (11E608c)
```

分析播放视频的时间区间: 22 FPS，GPU 9%



```c
  22  23931.0  FFmpegTutorial (64800) :0
  21 libdyld.dylib 23931.0  start
  20 FFmpegTutorial 23931.0  main /Users/qianlongxu/Documents/GitWorkSpace/StudyFFmpeg/Example/iOS/FFmpegTutorial-iOS/main.m:15
  19 UIKitCore 23931.0  UIApplicationMain
  18 GraphicsServices 23931.0  GSEventRunModal
  17 CoreFoundation 23931.0  CFRunLoopRunSpecific
  16 CoreFoundation 23927.0  __CFRunLoopRun
  15 CoreFoundation 14724.0  __CFRunLoopDoObservers
  14 CoreFoundation 14697.0  __CFRUNLOOP_IS_CALLING_OUT_TO_AN_OBSERVER_CALLBACK_FUNCTION__
  13 QuartzCore 14603.0  CA::Transaction::observer_callback(__CFRunLoopObserver*, unsigned long, void*)
  12 QuartzCore 14599.0  CA::Transaction::commit()
  11 QuartzCore 14563.0  CA::Context::commit_transaction(CA::Transaction*, double)
  10 QuartzCore 14519.0  CA::Layer::layout_and_display_if_needed(CA::Transaction*)
   9 QuartzCore 14487.0  -[CALayer _display]
   8 QuartzCore 14447.0  invocation function for block in CA::Layer::display_()
   7 QuartzCore 14447.0  CABackingStoreUpdate_
   6 CoreGraphics 13552.0  CGDisplayListDrawInContextDelegate
   5 CoreGraphics 13543.0  CG::DisplayList::executeEntries(std::__1::__wrap_iter<std::__1::unique_ptr<CG::DisplayListEntry const, std::__1::default_delete<CG::DisplayListEntry const> >*>, std::__1::__wrap_iter<std::__1::unique_ptr<CG::DisplayListEntry const, std::__1::default_delete<CG::DisplayListEntry const> >*>, CGContextDelegate*, CGRenderingState*, CGGStack*, CGRect const*, __CFDictionary const*, bool)
   4 CoreGraphics 12575.0  ripc_DrawImage
   3 CoreGraphics 10880.0  RIPLayerBltImage
   2 CoreGraphics 10876.0  ripl_Mark
   1 CoreGraphics 10876.0  argb32_image
   0 CoreGraphics 9724.0  argb32_sample_argb32
```





下一篇，我将给大家介绍如何使用 [Core Animation](./0x11.md) 避开后备存储器，复用画面以减少内存和 CPU 的使用！

